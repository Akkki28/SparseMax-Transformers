# SparseMax-Transformers
Built a transformer from scratch that uses a SparseMax Attention mechanism instead of the standard softmax attention used in the paper [attention is all you need](https://arxiv.org/abs/1706.03762)
